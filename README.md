# RAD System | Refund Abuse Detection

A rad system indeed.

A real-time refund abuse detection and decision support system for experience marketplaces. Built as a fully functional Streamlit prototype with a live scoring engine, LLM-assisted communication layer, and full audit trail. Not a demo â€” the engine computes live from the database for every assessment.

## Quick Start

### Local Development

```bash
git clone <repo-url>
cd projectRAD
pip install -r requirements.txt

# Optional: Add Groq API key for AI features
cp .env.example .env
# Edit .env and add your GROQ_API_KEY

streamlit run app.py
```

The system is fully functional without a Groq API key. AI-generated response scripts and evidence summaries will be unavailable, but all scoring, classification, and decision support features work.

### Streamlit Community Cloud

1. Fork this repository
2. Go to [share.streamlit.io](https://share.streamlit.io)
3. Deploy from the repo (main file: `app.py`)
4. Optionally add `GROQ_API_KEY` in the app's Secrets settings

The database generates automatically on first run. Writes during a session (decision log, profile updates) persist as long as the session is active. On Streamlit Cloud, these reset when the app sleeps after inactivity.

## How to Use

### 1. Guided Scenarios
Pick a scenario from the sidebar call queue. Each demonstrates a specific detection flow (auto-approve, escalation, vendor anomaly, etc.). The order ID is visible on each card â€” click to start.

### 2. Free Exploration
Enter any order ID from the database in the workspace input field. The system runs a live assessment against the full scoring engine. Browse available orders via the sidebar's "View All Orders" expander.

### 3. L1 â†’ L2 Flow
Process cases in the L1 dashboard. When you escalate, switch to the L2 Floor Manager tab to see the case with the full evidence packet, narrative summary, and resolution options.

## Guided Scenarios

| # | Customer | Scenario | Expected Outcome |
|---|----------|----------|-----------------|
| 1 | Sarah Mitchell | Vendor anomaly â€” Rome Colosseum (3 customers, same date) | ðŸŸ  Vendor anomaly detected |
| 2 | Priya Sharma | Clean customer, policy-compliant cancellation | ðŸŸ¢ Auto-approved |
| 3 | Tom Wallace | QR check-in contradicts no-show claim | ðŸ”´ Auto-flagged to L2 |
| 4 | Aisha Khan | Flagged customer + valid cancellation (policy overrides risk) | ðŸŸ¢ Auto-approved |
| 5 | Daniel Kim | Low-risk partial service claim | ðŸŸ¢ Low risk |
| 6 | Ananya Nair | Medium-risk no-show on non-cancelable product | ðŸŸ¡ Medium risk |
| 7 | Alex Drummond | Medium-risk technical issue, no QR data | ðŸŸ¡ Medium risk |
| 8 | James Liu | High-risk arbitrageur cancellation pattern | ðŸ”´ High risk |
| 9 | Lisa Chen | Repeat chancer, not-as-described pattern | ðŸ”´ High risk |
| 10 | Sophie Laurent | First-time customer, missing confirmation, no data | ðŸŸ¡ Medium risk (mitigated) |

## Architecture

The system uses a 4-layer deterministic engine. No LLM is involved in scoring or classification.

- **Layer 0 â€” Anomaly Check**: Detects experience-level refund clustering (vendor-side issues)
- **Layer 1 â€” Policy Gate**: Auto-approves policy-compliant requests; hard-flags QR contradictions and fraud flags
- **Layer 2 â€” Risk Profile**: 6-signal customer risk scoring with recency decay (refund frequency, no-show history, email engagement, timing, value, tenure)
- **Layer 3 â€” Request Evaluation**: Applies request-level modifiers (product type, timing, value, engagement, supplier context)

The LLM layer (Groq API) handles communication: response scripts, agent note extraction, evidence summaries, and contextual guidance. It never decides.

## Tech Stack

- **UI**: Streamlit
- **Data**: SQLite (generated on startup)
- **LLM**: Groq API â€” `llama-3.1-8b-instant` (scripts, notes, guidance) + `llama-3.3-70b-versatile` (evidence summaries)
- **Language**: Python

## Project Structure

```
projectRAD/
â”œâ”€â”€ app.py                      # Main Streamlit entry point
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ .env.example                # Template for GROQ_API_KEY
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generate_seed_data.py   # Database generation script
â”‚   â””â”€â”€ policies/
â”‚       â”œâ”€â”€ cancellation_policy.md
â”‚       â”œâ”€â”€ agent_response_guidelines.md
â”‚       â”œâ”€â”€ escalation_criteria.md
â”‚       â””â”€â”€ supplier_types_reference.md
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ config.py               # Configurable thresholds and weights
â”‚   â”œâ”€â”€ layer0_anomaly.py       # Experience-level anomaly detection
â”‚   â”œâ”€â”€ layer1_policy_gate.py   # Deterministic policy gate
â”‚   â”œâ”€â”€ layer2_risk_profile.py  # Customer risk profile scoring
â”‚   â”œâ”€â”€ layer3_request_eval.py  # Current request evaluation
â”‚   â”œâ”€â”€ classifier.py           # Final classification
â”‚   â””â”€â”€ profile_manager.py      # Profile CRUD and decision logging
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ response_generator.py   # L1 response script generation
â”‚   â”œâ”€â”€ note_extractor.py       # Agent note signal extraction
â”‚   â”œâ”€â”€ evidence_summarizer.py  # L2 evidence summarization
â”‚   â””â”€â”€ contextual_guidance.py  # Live case guidance
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ sidebar.py              # Call queue and scenario guide
â”‚   â”œâ”€â”€ l1_dashboard.py         # L1 conversational workspace
â”‚   â”œâ”€â”€ l2_dashboard.py         # L2 floor manager interface
â”‚   â”œâ”€â”€ system_overview.py      # Architecture, metrics, config
â”‚   â””â”€â”€ components.py           # Shared UI components
â””â”€â”€ utils/
    â”œâ”€â”€ db.py                   # Database connection helpers
    â””â”€â”€ policy_loader.py        # Policy document retrieval
```

## Configuration

All scoring thresholds and weights are in `engine/config.py`. You can tune them without code changes:

- Layer 0 anomaly threshold
- Layer 2 signal weights (sum to 100)
- Layer 2 risk thresholds and recency decay
- Layer 3 request modifiers
- Classification boundaries (low/medium/high)

## Note on Data

The prototype runs on synthetic seed data: 18 customers with ~250 booking records, designed to exercise every detection path. The data generation script (`data/generate_seed_data.py`) creates the database on startup. The database is ephemeral on Streamlit Cloud (resets when the app sleeps). Decision logs persist within a session but reset between sessions.
